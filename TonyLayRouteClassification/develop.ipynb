{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c220486d",
   "metadata": {},
   "source": [
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4626a",
   "metadata": {},
   "source": [
    "### Collecting hold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a437206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1b70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img,cvt=True):\n",
    "    if cvt:\n",
    "        if len(img.shape) == 2:\n",
    "            plt.imshow(img, cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "    \n",
    "# e.g. corners = [(2.0, 1.0), (4.0, 5.0), (7.0, 8.0)]\n",
    "def polygonal_area(corners):\n",
    "    n = len(corners) # of corners\n",
    "    area = 0.0\n",
    "    for i in range(n):\n",
    "        j = (i + 1) % n\n",
    "        area += corners[i][0] * corners[j][1]\n",
    "        area -= corners[j][0] * corners[i][1]\n",
    "    area = abs(area) / 2.0\n",
    "    return area\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path) as f:\n",
    "        json_data = json.load(f)\n",
    "        hold = json_data[\"shapes\"][0]\n",
    "        hold_area = polygonal_area(hold[\"points\"])\n",
    "        \n",
    "def show_image(img,cvt=True):\n",
    "    if cvt:\n",
    "        if len(img.shape) == 2:\n",
    "            cv2.imshow('img',img)\n",
    "        else:\n",
    "            cv2.imshow('img',cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "         cv2.imshow('img',img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()       \n",
    "    \n",
    "        \n",
    "def resize_image(image, width=750, inter=cv2.INTER_AREA):\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "    if width is None:\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "    else:\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    return cv2.resize(image, dim, interpolation=inter)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "688191bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot_image(img_src)\n",
    "#read_json('slab_images/image (1).json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a36287",
   "metadata": {},
   "source": [
    "### Climbing hold dot detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef20a026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported everything\n"
     ]
    }
   ],
   "source": [
    "import rgb_processing\n",
    "\n",
    "def ratio(n1, n2):\n",
    "  \"\"\"Calculates similarity score between 2 numbers\n",
    "  Args:\n",
    "      n1 (int): first number\n",
    "      n2 (int): second number\n",
    "  Returns:\n",
    "      (float): similarity score between the 2 numbers\n",
    "  \"\"\"\n",
    "  return 1 - abs(n1 - n2) / (n1 + n2)\n",
    "\n",
    "def subimage_from_coords(img_src,x1,y1,x2,y2):\n",
    "    return img_src[y1:y2,x1:x2]\n",
    "\n",
    "def percentage_dark(img_src,threshold):\n",
    "    num_dark = np.sum(img_src < threshold)\n",
    "    num_light =  np.sum(img_src >= threshold)\n",
    "    return num_dark/(num_light+num_dark)\n",
    "\n",
    "def find_dots(img_src):\n",
    "    \"\"\"Climbing dot detection using connected component analysis\n",
    "    Args:\n",
    "        img_src (Mat): the image source\n",
    "    Returns:\n",
    "        dot_centres (list): list of (x,y) coordinates of the climbing dots\n",
    "    \"\"\"    \n",
    "    # Create grayscale image\n",
    "    gray_img = cv2.cvtColor(img_src, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold search area\n",
    "    binary_img = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                       cv2.THRESH_BINARY_INV, 131, 15)\n",
    "    \n",
    "    # Blob detection\n",
    "    _, _, boxes, _ = cv2.connectedComponentsWithStats(binary_img)\n",
    "    \n",
    "    # First box is the background\n",
    "    boxes = boxes[1:]\n",
    "    \n",
    "    # For debugging\n",
    "#     debug_img = img_src.copy()\n",
    "#     for x,y,w,h,pixels in boxes:\n",
    "#         cv2.rectangle(debug_img, (x,y), (x+w,y+h), (0,255,0),2) # draw rectangle around the detected dots\n",
    "#     show_image(resize_image(debug_img,750),cvt=False)\n",
    "    \n",
    "    # Threshold blobs by pixel area and percentage of dark pixels\n",
    "    filtered_boxes = []\n",
    "    area_thres_lb = 80\n",
    "    while True:\n",
    "        for x,y,w,h,pixels in boxes:\n",
    "            area_limit = (area_thres_lb < pixels < 400)\n",
    "            subimage = subimage_from_coords(gray_img,x,y,x+w,y+h)\n",
    "\n",
    "            # Try thresholding by percentage of dark pixels\n",
    "            p = percentage_dark(subimage,75)\n",
    "            p_limit = 0.4 < p < 0.6\n",
    "            if p_limit and area_limit and ratio(w,h) > 0.75:\n",
    "                filtered_boxes.append((x,y,w,h))   \n",
    "                \n",
    "        if len(filtered_boxes) > 15:\n",
    "            break\n",
    "        else:\n",
    "            area_thres_lb -= 20\n",
    "            filtered_boxes = []\n",
    "            \n",
    "        if area_thres_lb < 0:\n",
    "            raise Exception(\"No dots can be detected in this image!\")\n",
    "    \n",
    "#     # Try Kmeans colour thresholding (look for main colour black) if the previous method didn't work\n",
    "#     if len(filtered_boxes) < 15:\n",
    "#         filtered_boxes = []\n",
    "        \n",
    "#     for x,y,w,h,pixels in boxes:\n",
    "#         area_limit = (80 < pixels < 400)\n",
    "#         subimage = subimage_from_coords(gray_img,x,y,x+w,y+h)\n",
    "        \n",
    "#         if area_limit and ratio(w,h) > 0.75:\n",
    "#             colour = rgb_processing.get_main_colour(subimage)\n",
    "#             if colour == 'black':\n",
    "#                 filtered_boxes.append((x,y,w,h))\n",
    "\n",
    "\n",
    "    # Create list of dot centres\n",
    "    dot_centres = []\n",
    "    for x,y,w,h in filtered_boxes:\n",
    "        cv2.rectangle(img_src, (x,y), (x+w,y+h), (0,255,0),2) # draw rectangle around the detected dots\n",
    "        dot_centre = (round(x+(w/2)),round(y+(h/2)))\n",
    "        dot_centres.append(dot_centre)\n",
    "    \n",
    "    #show_image(resize_image(img_src,300),cvt=False)\n",
    "    return dot_centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "378142f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cm/pixel: 0.39673953645581206\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "from statistics import mode\n",
    "from collections import defaultdict\n",
    "from math import degrees, atan2, sqrt\n",
    "\n",
    "def get_direction(c1,c2):\n",
    "    direction = abs(degrees(atan2(c2[1] - c1[1], c2[0] - c1[0])))\n",
    "    if 0 < direction < 10 or 85 < direction < 95 or 170 < direction < 180:\n",
    "        return 'straight'\n",
    "    elif 40 < direction < 50 or 130 < direction < 140:\n",
    "        return 'diagonal'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def modal_distance(dot_centres):\n",
    "    \"\"\"Returns the modal distance between a list of (x,y) coordinates\n",
    "    Args:\n",
    "        dot_centres (list): list of (x,y) coordinates of the climbing dots\n",
    "    Returns:\n",
    "        avg (int): the modal distance\n",
    "    \"\"\"\n",
    "    modal_distance_map = defaultdict(list)\n",
    "    direction_count = defaultdict(int)\n",
    "    distances = []\n",
    "    for dot_centre in dot_centres:\n",
    "        # Create list of coordinates that excludes the current iterated coordinate\n",
    "        other_centres = dot_centres\n",
    "        other_centres.remove(dot_centre)\n",
    "        # Kd-tree for quick nearest-neighbor lookup\n",
    "        tree = spatial.KDTree(other_centres)\n",
    "        # Append shortest distance to list of distances\n",
    "        dist, idx = tree.query(dot_centre)\n",
    "        rounded_dist = round(dist,-1)\n",
    "        modal_distance_map[rounded_dist].append(dist)\n",
    "        # Add to direction count dict\n",
    "        direction = get_direction(dot_centre,other_centres[idx])\n",
    "        direction_count[direction] += 1\n",
    "    \n",
    "    modal_distance = max(modal_distance_map, key=lambda key: len(modal_distance_map[key]))\n",
    "    dists = modal_distance_map[modal_distance]\n",
    "    \n",
    "    mean_dist = np.mean(dists)\n",
    "    #print(direction_count)\n",
    "    if direction_count['diagonal'] > direction_count['straight']:\n",
    "        dist = sqrt(mean_dist**2/2)*2\n",
    "    else:\n",
    "        dist = mean_dist\n",
    "        \n",
    "    scale = 15/dist\n",
    "    \n",
    "    if scale > 0.5:\n",
    "        raise Exception(f\"The scale value {scale} is too large, the dot detection is likely to be broken. \")\n",
    "    else:\n",
    "        return dist\n",
    "\n",
    "img_src = cv2.imread('../backend/services/files/wall11.jpg')\n",
    "dot_centres = find_dots(img_src)\n",
    "dist = modal_distance(dot_centres)\n",
    "cv2.line(img_src, (400,400), (400,400+round(dist)), (255,0,0), 5)\n",
    "show_image(resize_image(img_src))\n",
    "print(f\"cm/pixel: {15/dist}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d2715",
   "metadata": {},
   "source": [
    "### Route detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b08d3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack_V2-V4 (4)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../backend/services/files/wall11.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m data, hold_ids \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_route\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(hold_ids))\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m hold_ids:\n",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m, in \u001b[0;36mfilter_route\u001b[1;34m(file_name, colour, scale)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Retrieve colour of hold\u001b[39;00m\n\u001b[0;32m     49\u001b[0m hold_img \u001b[38;5;241m=\u001b[39m subimage_from_coords(extracted_hold,r_bb[\u001b[38;5;241m0\u001b[39m],r_bb[\u001b[38;5;241m1\u001b[39m],r_bb[\u001b[38;5;241m2\u001b[39m],r_bb[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m---> 50\u001b[0m hold_colour \u001b[38;5;241m=\u001b[39m \u001b[43mrgb_processing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_main_colour\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhold_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#print(hold_colour)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_colour \u001b[38;5;129;01min\u001b[39;00m hold_colour:\n",
      "File \u001b[1;32mc:\\Users\\bnroo\\OneDrive\\Desktop\\BoulderBrain\\bouldering_route_classification\\rgb_processing.py:139\u001b[0m, in \u001b[0;36mget_main_colour\u001b[1;34m(subimg)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the main colour of an image using KMeans clusteirng\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    subimg: img source\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    (str): colour\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m   \n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Convert image to RGB space\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m img_rgb \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m#show_image(img_rgb)\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Perform K-Means clustering to split image into 3 main colours\u001b[39;00m\n\u001b[0;32m    144\u001b[0m rgb_centres, idx_max \u001b[38;5;241m=\u001b[39m rgb_kmeans(img_rgb,num_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import rgb_processing\n",
    "import pickle\n",
    "\n",
    "classes = np.array(['jug','easy_foothold','sloper','large_sloper','easy_edge','crimp_edge','edge_large_sloper','crimp','double_sided_jug','pinch_sloper','foothold','double_sided_sloper','curve_edge_sloper','edge','pocket','pinch','edge_sloper','volume','large_moon_edge','platform','u_edge'])\n",
    "\n",
    "def cut_from_contour(img, contour):\n",
    "    mask = np.zeros(img.shape, np.uint8) # Create mask where white is what we want, black otherwise\n",
    "    cv2.drawContours(mask, contour, -1, (255,255,255), -1)\n",
    "    extracted_hold = np.zeros(img.shape, np.uint8) # Extract out the object and place into output image\n",
    "    extracted_hold[mask > 0] = img[mask > 0]   \n",
    "    return extracted_hold\n",
    "\n",
    "def load_data_from_pickle(file_name):\n",
    "    with open(f'tensor_data/{file_name}.pickle', 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    return data\n",
    "def filter_route(file_name,colour=\"\",scale=1):\n",
    "\n",
    "    # Load hold data from the file\n",
    "    data = load_data_from_pickle(file_name)\n",
    "    \n",
    "    # Load image file\n",
    "    img = cv2.imread('../backend/services/files/wall11.jpg')\n",
    "    \n",
    "    # Retrieve required colour\n",
    "    if not colour:\n",
    "        expected_colour = file_name.split(\"_\")[0]\n",
    "    else:\n",
    "        expected_colour = colour\n",
    "\n",
    "    # For debugging\n",
    "    img_detections = img.copy()\n",
    "    #print(len(data[\"contours\"]))\n",
    "    \n",
    "    # Retrieve indices of all holds that are relevant to the route by its colour\n",
    "    route_indices = []\n",
    "    for idx, bb in enumerate(data['pred_boxes']):\n",
    "        \n",
    "        contour = data['contours'][idx]\n",
    "        r_bb = np.round(bb).astype('int')\n",
    "        \n",
    "        # For debugging\n",
    "        cv2.rectangle(img_detections, (r_bb[0],r_bb[1]), (r_bb[2],r_bb[3]), (0,0,255),4)\n",
    "       \n",
    "        # Cut out hold\n",
    "        extracted_hold = cut_from_contour(img,contour)\n",
    "        \n",
    "        # Retrieve colour of hold\n",
    "        hold_img = subimage_from_coords(extracted_hold,r_bb[0],r_bb[1],r_bb[2],r_bb[3])\n",
    "        hold_colour = rgb_processing.get_main_colour(hold_img)\n",
    "        #print(hold_colour)\n",
    "        \n",
    "        if expected_colour in hold_colour:\n",
    "            route_indices.append(idx)\n",
    "    \n",
    "    if not route_indices:\n",
    "        raise Exception(f'Unable to detect any holds')\n",
    "    \n",
    "    # Filter out overlapping bounding boxes using non maximal suppression\n",
    "    boxes = [data['pred_boxes'][i] for i in route_indices]\n",
    "    confidences = [float(data['scores'][i]) for i in route_indices]\n",
    "    NMS_indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.7)\n",
    "    route_indices = [route_indices[i] for i in NMS_indices]\n",
    "    \n",
    "    # Scale the contour areas of the relevant holds\n",
    "    for i in route_indices:\n",
    "        data['contour_area'][i] *= scale\n",
    "    \n",
    "    #route_indices = [i for i in route_indices if data['contour_area'][i] > 1]\n",
    "        \n",
    "    # For debugging\n",
    "    #show_image(resize_image(img_detections,300),cvt=False)\n",
    "    return data, route_indices\n",
    "\n",
    "file_name = \"black_V2-V4 (4)\"\n",
    "img = cv2.imread('../backend/services/files/wall11.jpg')\n",
    "data, hold_ids = filter_route(file_name)\n",
    "print(len(hold_ids))\n",
    "for i in hold_ids:\n",
    "    r_bb = np.round(data['pred_boxes'][i]).astype('int')\n",
    "    cv2.rectangle(img, (r_bb[0],r_bb[1]), (r_bb[2],r_bb[3]), (0,0,255),4)\n",
    "plot_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b262017",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01db3702",
   "metadata": {},
   "source": [
    "### Route sequencing (distance between holds and direction between holds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import hypot, atan2, degrees\n",
    "\n",
    "def centres_from_boxes(bounding_boxes):\n",
    "    centres = [((bb[0]+bb[2])/2,(bb[1]+bb[3])/2) for bb in bounding_boxes]\n",
    "    return np.round(centres).astype('int')\n",
    "\n",
    "def get_route(hold_ids,hold_centres):\n",
    "    z = zip(hold_ids,hold_centres)\n",
    "    zs = sorted(z, key = lambda x: x[1][1],reverse=True)\n",
    "    sorted_ids, sorted_centres = map(list, zip(*zs))\n",
    "    return sorted_ids, sorted_centres\n",
    "\n",
    "def get_distance_direction(route,route_centres,scale):\n",
    "    distance_map = dict()   \n",
    "    direction_map = dict()\n",
    "    \n",
    "    for i, hold_id in enumerate(route):\n",
    "        c1 = route_centres[i]\n",
    "        if i < len(route)-1:\n",
    "            c2 = route_centres[i+1]\n",
    "            distance_map[hold_id] = hypot(c2[0] - c1[0], c2[1] - c1[1]) * scale\n",
    "            direction_map[hold_id] = abs(degrees(atan2(c2[1] - c1[1], c2[0] - c1[0])))\n",
    "        else:\n",
    "            distance_map[hold_id] = 0\n",
    "            direction_map[hold_id] = 0\n",
    "            \n",
    "    return distance_map, direction_map\n",
    "        \n",
    "        \n",
    "file_name = \"green_VB (32)\"\n",
    "img = cv2.imread('../backend/services/files/wall11.jpg')\n",
    "\n",
    "# Find dot centres to scale distances\n",
    "dot_centres = find_dots(img)\n",
    "dot_dist = modal_distance(dot_centres)\n",
    "scale = 15/dot_dist\n",
    "print(f\"Scale: {scale}\")\n",
    "\n",
    "# Retrieve only the holds that are relevant to the route\n",
    "data, hold_ids = filter_route(file_name,scale=scale)\n",
    "\n",
    "print(hold_ids)\n",
    "\n",
    "# Create centre coordinates for each hold\n",
    "bounding_boxes = [data['pred_boxes'][hold_id] for hold_id in hold_ids]\n",
    "hold_centres = centres_from_boxes(bounding_boxes)\n",
    "\n",
    "# Create mapping from hold id to its centre\n",
    "id_to_centre = dict()\n",
    "for i,hold_id in enumerate(hold_ids):\n",
    "    id_to_centre[hold_id] = hold_centres[i]\n",
    "    \n",
    "# Create a route as a sequence of holds sorted by y\n",
    "route, route_centres = get_route(hold_ids,hold_centres)\n",
    "print(f\"Route: {route}\")\n",
    "\n",
    "# Create a dict mapping from hold_id to its distance and direction to the next hold in the route sequence\n",
    "distance_map, direction_map = get_distance_direction(route,route_centres,scale)\n",
    "\n",
    "print(distance_map)\n",
    "print(direction_map)\n",
    "\n",
    "# Show the route\n",
    "temp = img.copy()\n",
    "for i,hold_id in enumerate(route):\n",
    "    text = f\"{str(i)} ({hold_id})\"\n",
    "    coords = id_to_centre[hold_id]\n",
    "    coords[0] -= 75\n",
    "    bb = np.round(data['pred_boxes'][hold_id]).astype('int')\n",
    "    cv2.putText(temp, text, coords, cv2.FONT_HERSHEY_TRIPLEX , 1.25, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    cv2.rectangle(temp, (bb[0],bb[1]), (bb[2],bb[3]), (0, 0, 255), 2)\n",
    "\n",
    "show_image(resize_image(temp,300),cvt=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e1e97",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db944e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "def check_num_samples(path):\n",
    "    class_count = defaultdict(int)\n",
    "    for file_path in glob.glob(path):\n",
    "        for cls in ['VB','V0-V2','V1-V3','V2-V4','V3-V5']:\n",
    "            if cls in file_path:\n",
    "                class_count[cls] += 1\n",
    "    print(class_count)\n",
    "    \n",
    "def get_route_images(path, classes, num_per_cls=16):\n",
    "    \n",
    "    route_files_by_class = defaultdict(list)\n",
    "    for file_path in glob.glob(path):\n",
    "        for cls in classes:\n",
    "            if cls in file_path:\n",
    "                file_name = os.path.basename(file_path)\n",
    "                route_files_by_class[cls].append(file_name)\n",
    "\n",
    "                \n",
    "    for cls in classes:\n",
    "        random.shuffle(route_files_by_class[cls])\n",
    "        route_files_by_class[cls] = route_files_by_class[cls][0:num_per_cls]\n",
    "        if len(route_files_by_class[cls]) != num_per_cls:\n",
    "            raise Exception(f\"The class {cls} does not have {num_per_cls} images in the dataset.\")\n",
    "    \n",
    "    return route_files_by_class\n",
    "\n",
    "def approximate_area_from_bb(bb):\n",
    "    x_len = bb[2]-bb[0]\n",
    "    y_len = bb[3]-bb[1]\n",
    "    return 0.5 * (x_len*y_len)\n",
    "\n",
    "# For debugging purposes\n",
    "def generate_datum(file_name):\n",
    "    print(f'Processing {file_name}')\n",
    "\n",
    "    # Find dot centres to define scale\n",
    "    img = cv2.imread('../backend/services/files/wall11')\n",
    "    scale = 15/modal_distance(find_dots(img))\n",
    "\n",
    "    # Retrieve only the holds that are relevant to the route\n",
    "    data, hold_ids = filter_route(os.path.basename(file_name).split(\".\")[0],scale=scale)\n",
    "\n",
    "    # Create centre coordinates for each hold\n",
    "    bounding_boxes = [data['pred_boxes'][hold_id] for hold_id in hold_ids]\n",
    "    hold_centres = centres_from_boxes(bounding_boxes)\n",
    "\n",
    "    # Create mapping from hold id to its centre\n",
    "    id_to_centre = dict()\n",
    "    for i,hold_id in enumerate(hold_ids):\n",
    "        id_to_centre[hold_id] = hold_centres[i]\n",
    "\n",
    "    # Create a route as a sequence of holds sorted by y\n",
    "    route, route_centres = get_route(hold_ids,hold_centres)\n",
    "\n",
    "    # Create a dict mapping from hold_id to its distance and direction to the next hold in the route sequence\n",
    "    distance_map, direction_map = get_distance_direction(route,route_centres,scale)\n",
    "\n",
    "    # Create feature data\n",
    "    feature_data = []\n",
    "    for hold_id in route:\n",
    "        cls = data['pred_classes'][hold_id]\n",
    "        contour_area = data['contour_area'][hold_id]\n",
    "        area = data['contour_area'][hold_id] * scale\n",
    "        distance_to_next_hold = distance_map[hold_id]\n",
    "        direction_to_next_hold = direction_map[hold_id]\n",
    "        feature_data.append([cls,area,distance_to_next_hold,direction_to_next_hold])    \n",
    "        \n",
    "    return(feature_data)\n",
    "    \n",
    "def generate_dataset(route_images_by_class):\n",
    "    dataset = []\n",
    "    for cls in classes:\n",
    "        for file_name in route_images_by_class[cls]:\n",
    "            print(f'Processing {file_name}')\n",
    "            \n",
    "            # Find dot centres to define scale\n",
    "            img = cv2.imread('../backend/services/files/wall11')\n",
    "            scale = 15/modal_distance(find_dots(img))\n",
    "            \n",
    "            # Retrieve only the holds that are relevant to the route\n",
    "            data, hold_ids = filter_route(os.path.basename(file_name).split(\".\")[0],scale=scale)\n",
    "\n",
    "            # Create centre coordinates for each hold\n",
    "            bounding_boxes = [data['pred_boxes'][hold_id] for hold_id in hold_ids]\n",
    "            hold_centres = centres_from_boxes(bounding_boxes)\n",
    "\n",
    "            # Create mapping from hold id to its centre\n",
    "            id_to_centre = dict()\n",
    "            for i,hold_id in enumerate(hold_ids):\n",
    "                id_to_centre[hold_id] = hold_centres[i]\n",
    "\n",
    "            # Create a route as a sequence of holds sorted by y\n",
    "            route, route_centres = get_route(hold_ids,hold_centres)\n",
    "\n",
    "            # Create a dict mapping from hold_id to its distance and direction to the next hold in the route sequence\n",
    "            distance_map, direction_map = get_distance_direction(route,route_centres,scale)\n",
    "            \n",
    "            # Create feature data\n",
    "            feature_data = []\n",
    "            for hold_id in route:\n",
    "                cls = data['pred_classes'][hold_id]\n",
    "                contour_area = data['contour_area'][hold_id]\n",
    "                \n",
    "                # There is a bug where OpenCV can't calculate the contour area and for some binary masks\n",
    "                # So approximate it from the bounding box\n",
    "                if contour_area < 1:\n",
    "                    area = approximate_area_from_bb(data['pred_boxes'][hold_id]) * scale\n",
    "                else:\n",
    "                    area = contour_area * scale\n",
    "                \n",
    "                distance_to_next_hold = distance_map[hold_id]\n",
    "                direction_to_next_hold = direction_map[hold_id]\n",
    "                feature_data.append([cls,area,distance_to_next_hold,direction_to_next_hold])\n",
    "            \n",
    "            dataset.append((file_name,feature_data))\n",
    "    \n",
    "    print('Finished!')\n",
    "    return dataset\n",
    "\n",
    "# datum = generate_datum(\"black_V2-V4 (4).jpg\")\n",
    "# print(datum)\n",
    "# print(len(datum))\n",
    "\n",
    "# ROUTE_IMAGES_PATH = ('route_images/*')\n",
    "\n",
    "# classes = ['VB','V1-V3','V2-V4','V3-V5']\n",
    "# check_num_samples(ROUTE_IMAGES_PATH)\n",
    "# route_images_by_class = get_route_images(ROUTE_IMAGES_PATH, classes, num_per_cls=15)\n",
    "# dataset = generate_dataset(route_images_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd898496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally cache the dataset\n",
    "cache = True\n",
    "if cache:\n",
    "    with open('dataset.pickle', 'wb') as handle:\n",
    "        pickle.dump(dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5466d0",
   "metadata": {},
   "source": [
    "### Normalising the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7569389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_value(v,min_v,max_v):\n",
    "    return (v - min_v) / (max_v - min_v)\n",
    "\n",
    "def normalise_hold(hold_type,num_classes):\n",
    "    return hold_type/num_classes\n",
    "\n",
    "def cls_from_route_label(name):\n",
    "    classes = ['VB','V1-V3','V2-V4','V3-V5']\n",
    "    for i,cls in enumerate(classes):\n",
    "        if cls in name:\n",
    "            return i\n",
    "    \n",
    "def normalise_dataset(dataset,num_classes):\n",
    "    \n",
    "    cls_labels = [cls_from_route_label(file_name) for file_name,_ in dataset]\n",
    "    dataset_without_files = [data for _,data in dataset]\n",
    "    \n",
    "    contour_areas = []\n",
    "    distances = []\n",
    "    directions = []\n",
    "    hold_counts = []\n",
    "    \n",
    "    # Create lists of all contour areas, distances and directions\n",
    "    for i,route_data in enumerate(dataset_without_files):\n",
    "        hold_count = 0\n",
    "        for hold in route_data:\n",
    "            contour_areas.append(hold[1])\n",
    "            distances.append(hold[2])\n",
    "            directions.append(hold[3])\n",
    "            hold_count +=1\n",
    "        hold_counts.append(hold_count)\n",
    "    \n",
    "    min_area = np.amin(contour_areas)\n",
    "    max_area = np.amax(contour_areas)\n",
    "    min_dist = np.amin(distances)\n",
    "    max_dist = np.amax(distances)\n",
    "    max_holds = np.amax(hold_counts)\n",
    "    \n",
    "    # Normalise the data\n",
    "    for i, route_data in enumerate(dataset_without_files):\n",
    "        for j, hold in enumerate(route_data):   \n",
    "            norm_hold = normalise_hold(hold[0],num_classes)\n",
    "            norm_area = normalise_value(hold[1],min_area,max_area)\n",
    "            norm_dist = normalise_value(hold[2],min_dist,max_dist)\n",
    "            norm_dir = normalise_value(hold[3],0,180)\n",
    "            normalised_data = [norm_hold,norm_area,norm_dist,norm_dir]\n",
    "            dataset_without_files[i][j] = normalised_data\n",
    "    \n",
    "    # Pad any routes until they reach the maximum hold count\n",
    "    for i in range(len(dataset_without_files)):\n",
    "        num_holds = len(dataset_without_files[i])\n",
    "        diff = abs(max_holds-num_holds)\n",
    "        if diff > 0:\n",
    "            for d in range(diff):\n",
    "                dataset_without_files[i].append([0,0,0,0])\n",
    "    \n",
    "    normalised_dataset = list(zip(dataset_without_files,cls_labels))\n",
    "    normalisation_values = [min_area,max_area,min_dist,max_dist,max_holds,num_classes]\n",
    "    return normalised_dataset, normalisation_values\n",
    "\n",
    "# Debugging purposes\n",
    "#     print(f'Minimum area: {np.amin(contour_areas)}')\n",
    "#     print(f'Minimum distance: {np.amin(distances)}')\n",
    "#     print(f'Minimum direction: {np.amin(directions)}')\n",
    "#     print(f'Maximum area: {np.amax(contour_areas)}')\n",
    "#     print(f'Maximum distance: {np.amax(distances)}')\n",
    "#     print(f'Maximum direction: {np.amax(directions)}')\n",
    "    \n",
    "    \n",
    "\n",
    "# Load dataset from cache\n",
    "with open('dataset.pickle', 'rb') as handle:\n",
    "    dataset = pickle.load(handle)\n",
    "\n",
    "hold_classes = ['jug','easy_foothold','sloper','large_sloper','easy_edge','crimp_edge','edge_large_sloper','crimp','double_sided_jug','pinch_sloper','foothold','double_sided_sloper','curve_edge_sloper','edge','pocket','pinch','edge_sloper','volume','large_moon_edge','platform','u_edge']\n",
    "normalised_dataset, norm_values = normalise_dataset(dataset,len(hold_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalised_dataset[i] returns ([data],label) for the sample at i\n",
    "X = np.array([data for data,_ in normalised_dataset])\n",
    "y = np.array([label for _,label in normalised_dataset])\n",
    "\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8175a84f",
   "metadata": {},
   "source": [
    "### Build Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6acea948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(32, input_shape=(X_train.shape[1:]), activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d25dfc",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ce708",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          epochs=50,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b86f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "route_classes = ['VB','V1-V3','V2-V4','V3-V5']\n",
    "\n",
    "# Single sample prediction\n",
    "sample_num = 47\n",
    "sample = X_train[sample_num:sample_num+1]\n",
    "print(sample)\n",
    "prediction = model.predict(sample)\n",
    "print(prediction)\n",
    "\n",
    "expected_class = route_classes[y_train[sample_num]]\n",
    "class_prediction = route_classes[np.argmax(prediction)]\n",
    "print(f'Predicted: {class_prediction}, Actual: {expected_class}')\n",
    "\n",
    "# Accuracy on test set\n",
    "num_correct = 0\n",
    "num_total = X_test.shape[0]\n",
    "for i in range(num_total):\n",
    "    sample = X_test[i:i+1]\n",
    "    prediction = model.predict(sample)\n",
    "    class_prediction = np.argmax(prediction)\n",
    "    expected_class = y_test[i]\n",
    "    if class_prediction == expected_class:\n",
    "        num_correct += 1\n",
    "print(f'Test Accuracy: {(num_correct/num_total)*100}%')\n",
    "\n",
    "# Accuracy on train set\n",
    "num_correct = 0\n",
    "num_total = X_train.shape[0]\n",
    "for i in range(num_total):\n",
    "    sample = X_train[i:i+1]\n",
    "    prediction = model.predict(sample)\n",
    "    class_prediction = np.argmax(prediction)\n",
    "    expected_class = y_train[i]\n",
    "    if class_prediction == expected_class:\n",
    "        num_correct += 1\n",
    "print(f'Train Accuracy: {(num_correct/num_total)*100}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524679a",
   "metadata": {},
   "source": [
    "### Prediction from image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac42c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging purposes\n",
    "def generate_datum_norm(file_name,norm_values,num_classes):\n",
    "    min_area = norm_values[0]\n",
    "    max_area = norm_values[1]\n",
    "    min_dist = norm_values[2]\n",
    "    max_dist = norm_values[3]\n",
    "    max_holds = norm_values[4]\n",
    "    \n",
    "    print(f'Processing {file_name}')\n",
    "\n",
    "    # Find dot centres to define scale\n",
    "    img = cv2.imread('../backend/services/files/wall11')\n",
    "    scale = 15/modal_distance(find_dots(img))\n",
    "\n",
    "    # Retrieve only the holds that are relevant to the route\n",
    "    data, hold_ids = filter_route(os.path.basename(file_name).split(\".\")[0],scale=scale)\n",
    "\n",
    "    # Create centre coordinates for each hold\n",
    "    bounding_boxes = [data['pred_boxes'][hold_id] for hold_id in hold_ids]\n",
    "    hold_centres = centres_from_boxes(bounding_boxes)\n",
    "\n",
    "    # Create mapping from hold id to its centre\n",
    "    id_to_centre = dict()\n",
    "    for i,hold_id in enumerate(hold_ids):\n",
    "        id_to_centre[hold_id] = hold_centres[i]\n",
    "\n",
    "    # Create a route as a sequence of holds sorted by y\n",
    "    route, route_centres = get_route(hold_ids,hold_centres)\n",
    "\n",
    "    # Create a dict mapping from hold_id to its distance and direction to the next hold in the route sequence\n",
    "    distance_map, direction_map = get_distance_direction(route,route_centres,scale)\n",
    "\n",
    "    # Create normalised feature data\n",
    "    feature_data = []\n",
    "    for hold_id in route:\n",
    "        cls = data['pred_classes'][hold_id] / num_classes\n",
    "        contour_area = normalise_value(data['contour_area'][hold_id]*scale,min_area,max_area)\n",
    "        distance_to_next_hold = normalise_value(distance_map[hold_id]*scale,min_dist,max_dist)\n",
    "        direction_to_next_hold = normalise_value(direction_map[hold_id]*scale,0,180)\n",
    "        feature_data.append([cls,contour_area,distance_to_next_hold,direction_to_next_hold])\n",
    "    \n",
    "    # Pad route until it reaches the maximum hold count\n",
    "    num_holds = len(feature_data)\n",
    "    diff = abs(max_holds-num_holds)\n",
    "    if diff > 0:\n",
    "        for d in range(diff):\n",
    "            feature_data.append([0,0,0,0]) \n",
    "            \n",
    "    return feature_data, cls_from_route_label(file_name)\n",
    "\n",
    "def predict_from_image(model,file_name,norm_values,num_classes):\n",
    "    X, y = generate_datum_norm(file_name,norm_values,num_classes)\n",
    "    X = np.array(X)\n",
    "    X = X[None, :, :]\n",
    "    print(X.shape)\n",
    "    \n",
    "    route_classes = ['VB','V1-V3','V2-V4','V3-V5']\n",
    "    \n",
    "    prediction = model.predict(X)\n",
    "    expected_class = route_classes[y]\n",
    "    class_prediction = route_classes[np.argmax(prediction)]\n",
    "    print(f'Predicted: {class_prediction}, Actual: {expected_class}')\n",
    "    \n",
    "\n",
    "file_name = \"blue_VB.jpg\"\n",
    "predict_from_image(model,file_name,norm_values,22)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a4655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
